base_model: LGAI-EXAONE/EXAONE-4.0-1.2B
output_dir: outputs/lora
train:
  num_train_epochs: 1
  per_device_train_batch_size: 1
  learning_rate: 1.0e-4
  max_seq_length: 2048
  gradient_accumulation_steps: 8
  fp16: false
  bf16: true
lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
