# EXAONE Compression Toolkit

Official baseline source for this repo is `open/base_model` (from the organizer-provided `open.zip`).
All baseline-dependent commands are aligned to that path.

## í•œêµ­ì–´ (KOR)

### 1) ë¬¸ì œì™€ ì›ì¸
ê¸°ì¡´ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸(`scripts/08_eval_compare.py`)ëŠ” `llm.generate()`ì— plain prompt ë¬¸ìžì—´ì„ ì§ì ‘ ë„£ê³  ìžˆì—ˆìŠµë‹ˆë‹¤.
EXAONE ê³„ì—´(íŠ¹ížˆ ì••ì¶•/ì–‘ìží™” ë³€í˜• í¬í•¨)ì€ ì´ ì„¤ì •ì—ì„œ EOSë¡œ ì¦‰ì‹œ ì¢…ë£Œë˜ëŠ” ê²½ìš°ê°€ ë§Žì•„, ë‹¤ìˆ˜ í”„ë¡¬í”„íŠ¸ê°€ ë¹ˆ ë¬¸ìžì—´(`''`)ì´ ë˜ëŠ” ë¬¸ì œê°€ ìžˆì—ˆìŠµë‹ˆë‹¤.

### 2) ì ìš©í•œ ìˆ˜ì • (Refactor)
ë¹ˆ ì¶œë ¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì•„ëž˜ë¥¼ ì½”ë“œì— ë°˜ì˜í–ˆìŠµë‹ˆë‹¤.

1. `scripts/08_eval_compare.py`
- ìƒì„± ê²½ë¡œë¥¼ `generate()` -> `chat()`ë¡œ ë³€ê²½
- í”„ë¡¬í”„íŠ¸ë¥¼ chat ë©”ì‹œì§€ í˜•íƒœ(`[{"role":"user","content":...}]`)ë¡œ ì „ë‹¬
- ê¸°ë³¸ ìƒ˜í”Œë§ì„ `temperature=0.0`, `min_tokens=8`, `max_tokens=64`ë¡œ ê³ ì •
- `non_empty_count`, `non_empty_rate` ì§€í‘œë¥¼ ë¦¬í¬íŠ¸ì— ì¶”ê°€

2. `scripts/02_verify_vllm.py`
- ê²€ì¦ë„ ë™ì¼í•˜ê²Œ `chat()` ê²½ë¡œ ì‚¬ìš©
- ë¹ˆ ë¬¸ìžì—´ì´ë©´ ì¦‰ì‹œ ì‹¤íŒ¨í•˜ë„ë¡ ê°•ì œ (`Generated empty output.`)

### 3) ìˆ˜ì • í›„ ìž¬ê²€ì¦ ê²°ê³¼ (8ê°œ ëª¨ë¸)
ë¦¬í¬íŠ¸ íŒŒì¼:
- `/workspace/exaone-compression-clean/outputs/eval_compare_all_8cases_chatfix.json`

í•µì‹¬ ê²°ê³¼:
- **baseline + 7 candidates ì „ë¶€ non_empty_rate = 1.0**
- ì¦‰, 10ê°œ ê³ ì • í”„ë¡¬í”„íŠ¸ ê¸°ì¤€ **ëª¨ë“  ëª¨ë¸ì´ ë¹ˆ ì¶œë ¥ ì—†ì´ ìƒì„± ì„±ê³µ**

| Model | Elapsed (s) | Speedup | Avg Similarity | Exact Match | Non-Empty Rate |
|---|---:|---:|---:|---:|---:|
| `open/base_model` | 27.146 | 1.0000 | 1.0000 | 1.0 | 1.0 |
| `models/base-distilled` | 15.505 | 1.7508 | 0.9691 | 0.9 | 1.0 |
| `models/compressed-l29` | 15.234 | 1.7819 | 0.4965 | 0.0 | 1.0 |
| `models/base-llmc-awq` | 17.166 | 1.5814 | 0.5420 | 0.0 | 1.0 |
| `models/compressed-l29-distilled` | 11.379 | 2.3856 | 0.4777 | 0.0 | 1.0 |
| `models/compressed-l29-llmc-awq` | 10.564 | 2.5697 | 0.5155 | 0.1 | 1.0 |
| `models/base-distilled-llmc-awq` | 11.221 | 2.4192 | 0.5359 | 0.1 | 1.0 |
| `models/compressed-l29-distilled-llmc-awq` | 10.688 | 2.5399 | 0.5148 | 0.1 | 1.0 |

ì°¸ê³ :
- ê¸°ì¡´ empty-output ê¸°ë°˜ ì ìˆ˜ ì™œê³¡ì€ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.
- ì§€ê¸ˆì˜ similarity/exactëŠ” "ì •ìƒ ë¬¸ìž¥ë¼ë¦¬ ë¹„êµ"ê°€ ë˜ë¯€ë¡œ ì´ì „ë³´ë‹¤ ì˜ë¯¸ ìžˆëŠ” í”„ë¡ì‹œìž…ë‹ˆë‹¤.

### 4) ìƒì„± ì˜ˆì‹œ (ìˆ˜ì • í›„)
Prompt: `Explain model compression in one paragraph.`
- `open/base_model`: `Model compression is the technique used to reduce the size ...`
- `models/base-llmc-awq`: `Model compression is the process of reducing the size ...`
- `models/compressed-l29-llmc-awq`: `Model compression is a technique used in deep learning ...`

Prompt: `hello`
- `open/base_model`: `Hello! ðŸ˜Š How can I assist you today?`
- `models/compressed-l29`: `Hello! How can I assist you today?`
- `models/base-llmc-awq`: `Hello! ðŸ˜Š How can I help you today?`

### 5) ì‹¤í–‰ ë°©ë²•
```bash
# 8ê°œ ëª¨ë¸ ë¹„êµ (chat-fix ê¸°ë³¸ê°’ ì‚¬ìš©)
uv run python scripts/08_eval_compare.py \
  --baseline-model open/base_model \
  --candidate-models \
    models/base-distilled \
    models/compressed-l29 \
    models/base-llmc-awq \
    models/compressed-l29-distilled \
    models/compressed-l29-llmc-awq \
    models/base-distilled-llmc-awq \
    models/compressed-l29-distilled-llmc-awq \
  --report-file outputs/eval_compare_all_8cases_chatfix.json
```

```bash
# ë‹¨ì¼ ëª¨ë¸ ê²€ì¦ (ë¹ˆ ë¬¸ìžì—´ ì¶œë ¥ ì‹œ ì‹¤íŒ¨)
uv run python scripts/02_verify_vllm.py \
  --model-dir models/base-llmc-awq \
  --prompt "Explain model compression in one short paragraph." \
  --report-file outputs/verify_base_llmc_awq_chatfix.json
```

---

## English (ENG)

### 1) Problem and Root Cause
The previous eval path used `llm.generate()` with plain prompts.
For EXAONE variants (especially compressed/quantized), that frequently caused immediate EOS and empty outputs.
The organizer-provided baseline path in this repo is `open/base_model`.

### 2) Refactor Applied
1. `scripts/08_eval_compare.py`
- Switched generation from `generate()` to `chat()`
- Uses chat messages (`[{"role":"user","content":...}]`)
- Default sampling: `temperature=0.0`, `min_tokens=8`, `max_tokens=64`
- Added `non_empty_count` and `non_empty_rate` to the report

2. `scripts/02_verify_vllm.py`
- Also switched to `chat()`
- Hard-fails on empty generation (`Generated empty output.`)

### 3) Post-fix Validation
Report:
- `/workspace/exaone-compression-clean/outputs/eval_compare_all_8cases_chatfix.json`

Result:
- **All 8 models achieved `non_empty_rate = 1.0` on all 10 fixed prompts**.
- Empty-string collapse is resolved.

### 4) Notes
- Similarity/exact-match now compare non-empty outputs, so they are more meaningful than before.
- The metric is still a proxy; final leaderboard quality should be judged on task-specific benchmark outputs.

### 5) Stronger Recovery + Better Evaluation (New)
To improve leaderboard potential above low scores, this repo now includes:
- LoRA-based distillation recovery for pruned model (`scripts/03_lora_train.py`)
- LoRA merge to standalone HF checkpoint (`scripts/04_merge_lora.py`)
- Benchmark-style quality evaluation (`scripts/09_eval_quality.py`)
- Token-level speed evaluation aligned with `sec/token` normalization (`scripts/10_eval_token_latency.py`)

Run:
```bash
# 1) Build pruned model first
make compress

# 2) Strong recovery: LoRA distill + merge
make recover-lora

# 3) Evaluate quality proxy (MMLU + BoolQ)
make eval-quality

# 4) Evaluate speed proxy (token latency / SpeedNorm-like)
make eval-token-speed
```

If `peft` is missing:
```bash
uv pip install 'peft>=0.17,<0.18'
```
